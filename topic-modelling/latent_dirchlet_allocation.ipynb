{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"abcnews-date-text.csv\", error_bad_lines=False)\n",
    "data_text = data[ : 300000][['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>broughton hall audit reveals serious breaches</td>\n",
       "      <td>299995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>broughton hall fails key standards</td>\n",
       "      <td>299996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>broughton hall safe for residents govt says</td>\n",
       "      <td>299997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>burn off at conservation park aims to prevent</td>\n",
       "      <td>299998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>burns suspended for two games</td>\n",
       "      <td>299999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline_text   index\n",
       "0       aba decides against community broadcasting lic...       0\n",
       "1          act fire witnesses must be aware of defamation       1\n",
       "2          a g calls for infrastructure protection summit       2\n",
       "3                air nz staff in aust strike for pay rise       3\n",
       "4           air nz strike to affect australian travellers       4\n",
       "...                                                   ...     ...\n",
       "299995      broughton hall audit reveals serious breaches  299995\n",
       "299996                 broughton hall fails key standards  299996\n",
       "299997        broughton hall safe for residents govt says  299997\n",
       "299998      burn off at conservation park aims to prevent  299998\n",
       "299999                      burns suspended for two games  299999\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gensim and NLTK libraries\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('../../../../Python Libs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_words</th>\n",
       "      <th>singles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_words singles\n",
       "0        caresses  caress\n",
       "1           flies     fli\n",
       "2            dies     die\n",
       "3           mules    mule\n",
       "4          denied    deni\n",
       "5            died     die\n",
       "6          agreed    agre\n",
       "7           owned     own\n",
       "8         humbled   humbl\n",
       "9           sized    size\n",
       "10        meeting    meet\n",
       "11        stating   state\n",
       "12        siezing    siez\n",
       "13    itemization    item\n",
       "14    sensational  sensat\n",
       "15    traditional  tradit\n",
       "16      reference   refer\n",
       "17      colonizer   colon\n",
       "18        plotted    plot"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data={\"original_words\": original_words, \"singles\": singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_string(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text) :\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_string(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document : \n",
      "['rain', 'helps', 'dampen', 'bushfires']\n",
      "\n",
      "\n",
      " Tokenized and Lemmatized document : \n",
      "['rain', 'help', 'dampen', 'bushfir']\n"
     ]
    }
   ],
   "source": [
    "document_num = 4310\n",
    "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original Document : \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\n Tokenized and Lemmatized document : \")\n",
    "print(preprocess(text=doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rain helps dampen bushfires'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[documents['index'] == document_num].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = documents['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for (k, v) in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(71, 1), (107, 1), (462, 1), (3530, 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 71 (\"bushfir\") appears 1 times\n",
      "Word 107 (\"help\") appears 1 times\n",
      "Word 462 (\"rain\") appears 1 times\n",
      "Word 3530 (\"dampen\") appears 1 times\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(0, len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} times\".format(bow_doc_4310[i][0], dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF on doc set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tf_idf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959813347777092),\n",
      " (1, 0.39204529549491984),\n",
      " (2, 0.48531419274988147),\n",
      " (3, 0.5055461098578569)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "#Run only first time\n",
    "if train:\n",
    "    lda_model = models.LdaMulticore(corpus=bow_corpus,\n",
    "                            num_topics=10,\n",
    "                            id2word=dictionary,\n",
    "                            passes=2\n",
    "                           )\n",
    "    lda_model.save(\"./model/lda_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = models.LdaModel.load(\"./model/lda_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic : 0.015*\"murder\" + 0.012*\"jail\" + 0.010*\"year\" + 0.010*\"drought\" + 0.009*\"plan\" + 0.009*\"charg\" + 0.009*\"report\" + 0.009*\"releas\" + 0.009*\"clear\" + 0.008*\"court\" \n",
      "Words : 0\n",
      "\n",
      "\n",
      "Topic : 0.043*\"polic\" + 0.019*\"kill\" + 0.017*\"investig\" + 0.012*\"attack\" + 0.012*\"probe\" + 0.011*\"bomb\" + 0.011*\"secur\" + 0.009*\"victim\" + 0.008*\"seek\" + 0.008*\"accid\" \n",
      "Words : 1\n",
      "\n",
      "\n",
      "Topic : 0.018*\"miss\" + 0.013*\"search\" + 0.012*\"polic\" + 0.010*\"take\" + 0.008*\"guilti\" + 0.008*\"toll\" + 0.007*\"win\" + 0.007*\"death\" + 0.007*\"head\" + 0.006*\"tiger\" \n",
      "Words : 2\n",
      "\n",
      "\n",
      "Topic : 0.020*\"council\" + 0.015*\"polic\" + 0.015*\"drug\" + 0.013*\"court\" + 0.012*\"elect\" + 0.011*\"seek\" + 0.009*\"water\" + 0.009*\"urg\" + 0.008*\"green\" + 0.008*\"charg\" \n",
      "Words : 3\n",
      "\n",
      "\n",
      "Topic : 0.011*\"protest\" + 0.008*\"market\" + 0.008*\"melbourn\" + 0.007*\"prompt\" + 0.007*\"rais\" + 0.007*\"gold\" + 0.006*\"bush\" + 0.006*\"urg\" + 0.005*\"concern\" + 0.005*\"price\" \n",
      "Words : 4\n",
      "\n",
      "\n",
      "Topic : 0.021*\"fund\" + 0.019*\"boost\" + 0.019*\"govt\" + 0.011*\"offer\" + 0.011*\"consid\" + 0.010*\"indigen\" + 0.010*\"school\" + 0.009*\"push\" + 0.009*\"child\" + 0.008*\"worker\" \n",
      "Words : 5\n",
      "\n",
      "\n",
      "Topic : 0.020*\"plan\" + 0.017*\"water\" + 0.015*\"council\" + 0.012*\"centr\" + 0.009*\"claim\" + 0.009*\"begin\" + 0.007*\"union\" + 0.007*\"worker\" + 0.006*\"rise\" + 0.006*\"land\" \n",
      "Words : 6\n",
      "\n",
      "\n",
      "Topic : 0.016*\"health\" + 0.014*\"govt\" + 0.012*\"warn\" + 0.010*\"closer\" + 0.009*\"england\" + 0.008*\"say\" + 0.008*\"forc\" + 0.007*\"world\" + 0.007*\"black\" + 0.006*\"face\" \n",
      "Words : 7\n",
      "\n",
      "\n",
      "Topic : 0.014*\"hospit\" + 0.014*\"final\" + 0.013*\"test\" + 0.010*\"charg\" + 0.009*\"play\" + 0.008*\"open\" + 0.007*\"say\" + 0.007*\"lead\" + 0.007*\"terror\" + 0.007*\"face\" \n",
      "Words : 8\n",
      "\n",
      "\n",
      "Topic : 0.023*\"urg\" + 0.017*\"govt\" + 0.013*\"plan\" + 0.011*\"chang\" + 0.010*\"iraq\" + 0.010*\"troop\" + 0.010*\"support\" + 0.009*\"talk\" + 0.008*\"stand\" + 0.008*\"nuclear\" \n",
      "Words : 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic : {} \\nWords : {}\".format(topic, idx))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    lda_model_tfidf = models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                      num_topics=10,\n",
    "                                      id2word=dictionary,\n",
    "                                      passes=2\n",
    "                                     )\n",
    "    lda_model_tfidf.save(\"./model/lda_model_tfidf\")\n",
    "lda_model_tfidf = models.LdaModel.load(\"./model/lda_model_tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic : 0.008*\"iraq\" + 0.007*\"kill\" + 0.007*\"troop\" + 0.006*\"attack\" + 0.006*\"plan\" + 0.006*\"soldier\" + 0.006*\"govt\" + 0.004*\"polic\" + 0.004*\"test\" + 0.004*\"afghanistan\" \n",
      "Words : 0\n",
      "\n",
      "\n",
      "Topic : 0.006*\"polic\" + 0.006*\"charg\" + 0.005*\"care\" + 0.005*\"climat\" + 0.005*\"child\" + 0.004*\"seek\" + 0.004*\"violenc\" + 0.004*\"govt\" + 0.004*\"health\" + 0.004*\"bail\" \n",
      "Words : 1\n",
      "\n",
      "\n",
      "Topic : 0.006*\"polic\" + 0.005*\"assault\" + 0.005*\"govt\" + 0.005*\"station\" + 0.004*\"school\" + 0.004*\"council\" + 0.004*\"plan\" + 0.004*\"nurs\" + 0.004*\"stand\" + 0.004*\"candid\" \n",
      "Words : 2\n",
      "\n",
      "\n",
      "Topic : 0.018*\"crash\" + 0.014*\"polic\" + 0.013*\"kill\" + 0.007*\"investig\" + 0.007*\"charg\" + 0.007*\"court\" + 0.007*\"accid\" + 0.006*\"fatal\" + 0.006*\"bomb\" + 0.006*\"death\" \n",
      "Words : 3\n",
      "\n",
      "\n",
      "Topic : 0.007*\"charg\" + 0.007*\"polic\" + 0.006*\"drive\" + 0.006*\"drink\" + 0.004*\"nuclear\" + 0.004*\"plan\" + 0.004*\"face\" + 0.004*\"govt\" + 0.004*\"kangaroo\" + 0.004*\"right\" \n",
      "Words : 4\n",
      "\n",
      "\n",
      "Topic : 0.006*\"murder\" + 0.006*\"coast\" + 0.006*\"guilti\" + 0.006*\"govt\" + 0.005*\"plead\" + 0.005*\"west\" + 0.004*\"water\" + 0.004*\"urg\" + 0.004*\"plan\" + 0.004*\"charg\" \n",
      "Words : 5\n",
      "\n",
      "\n",
      "Topic : 0.019*\"closer\" + 0.011*\"water\" + 0.008*\"rain\" + 0.007*\"drought\" + 0.006*\"govt\" + 0.005*\"council\" + 0.005*\"farmer\" + 0.005*\"hick\" + 0.004*\"restrict\" + 0.004*\"plan\" \n",
      "Words : 6\n",
      "\n",
      "\n",
      "Topic : 0.007*\"miss\" + 0.006*\"market\" + 0.006*\"search\" + 0.005*\"push\" + 0.005*\"public\" + 0.005*\"rise\" + 0.005*\"record\" + 0.005*\"govt\" + 0.004*\"plan\" + 0.004*\"price\" \n",
      "Words : 7\n",
      "\n",
      "\n",
      "Topic : 0.009*\"blaze\" + 0.006*\"plan\" + 0.006*\"firefight\" + 0.006*\"council\" + 0.005*\"govt\" + 0.005*\"polic\" + 0.005*\"crew\" + 0.004*\"call\" + 0.004*\"titl\" + 0.004*\"develop\" \n",
      "Words : 8\n",
      "\n",
      "\n",
      "Topic : 0.005*\"say\" + 0.005*\"govt\" + 0.004*\"fund\" + 0.004*\"council\" + 0.004*\"cancer\" + 0.004*\"open\" + 0.004*\"australia\" + 0.004*\"patient\" + 0.004*\"lead\" + 0.004*\"urg\" \n",
      "Words : 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic : {} \\nWords : {}\".format(topic, idx))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'help', 'dampen', 'bushfir']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_docs[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8199232220649719\t \n",
      "Topic: 0.020*\"plan\" + 0.017*\"water\" + 0.015*\"council\" + 0.012*\"centr\" + 0.009*\"claim\" + 0.009*\"begin\" + 0.007*\"union\" + 0.007*\"worker\" + 0.006*\"rise\" + 0.006*\"land\"\n",
      "\n",
      "Score: 0.020015785470604897\t \n",
      "Topic: 0.011*\"protest\" + 0.008*\"market\" + 0.008*\"melbourn\" + 0.007*\"prompt\" + 0.007*\"rais\" + 0.007*\"gold\" + 0.006*\"bush\" + 0.006*\"urg\" + 0.005*\"concern\" + 0.005*\"price\"\n",
      "\n",
      "Score: 0.020013196393847466\t \n",
      "Topic: 0.021*\"fund\" + 0.019*\"boost\" + 0.019*\"govt\" + 0.011*\"offer\" + 0.011*\"consid\" + 0.010*\"indigen\" + 0.010*\"school\" + 0.009*\"push\" + 0.009*\"child\" + 0.008*\"worker\"\n",
      "\n",
      "Score: 0.0200100876390934\t \n",
      "Topic: 0.020*\"council\" + 0.015*\"polic\" + 0.015*\"drug\" + 0.013*\"court\" + 0.012*\"elect\" + 0.011*\"seek\" + 0.009*\"water\" + 0.009*\"urg\" + 0.008*\"green\" + 0.008*\"charg\"\n",
      "\n",
      "Score: 0.020007522776722908\t \n",
      "Topic: 0.023*\"urg\" + 0.017*\"govt\" + 0.013*\"plan\" + 0.011*\"chang\" + 0.010*\"iraq\" + 0.010*\"troop\" + 0.010*\"support\" + 0.009*\"talk\" + 0.008*\"stand\" + 0.008*\"nuclear\"\n",
      "\n",
      "Score: 0.020007511600852013\t \n",
      "Topic: 0.043*\"polic\" + 0.019*\"kill\" + 0.017*\"investig\" + 0.012*\"attack\" + 0.012*\"probe\" + 0.011*\"bomb\" + 0.011*\"secur\" + 0.009*\"victim\" + 0.008*\"seek\" + 0.008*\"accid\"\n",
      "\n",
      "Score: 0.02000722661614418\t \n",
      "Topic: 0.015*\"murder\" + 0.012*\"jail\" + 0.010*\"year\" + 0.010*\"drought\" + 0.009*\"plan\" + 0.009*\"charg\" + 0.009*\"report\" + 0.009*\"releas\" + 0.009*\"clear\" + 0.008*\"court\"\n",
      "\n",
      "Score: 0.020006755366921425\t \n",
      "Topic: 0.018*\"miss\" + 0.013*\"search\" + 0.012*\"polic\" + 0.010*\"take\" + 0.008*\"guilti\" + 0.008*\"toll\" + 0.007*\"win\" + 0.007*\"death\" + 0.007*\"head\" + 0.006*\"tiger\"\n",
      "\n",
      "Score: 0.020004475489258766\t \n",
      "Topic: 0.014*\"hospit\" + 0.014*\"final\" + 0.013*\"test\" + 0.010*\"charg\" + 0.009*\"play\" + 0.008*\"open\" + 0.007*\"say\" + 0.007*\"lead\" + 0.007*\"terror\" + 0.007*\"face\"\n",
      "\n",
      "Score: 0.020004209131002426\t \n",
      "Topic: 0.016*\"health\" + 0.014*\"govt\" + 0.012*\"warn\" + 0.010*\"closer\" + 0.009*\"england\" + 0.008*\"say\" + 0.008*\"forc\" + 0.007*\"world\" + 0.007*\"black\" + 0.006*\"face\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.632253110408783\t \n",
      "Topic: 0.019*\"closer\" + 0.011*\"water\" + 0.008*\"rain\" + 0.007*\"drought\" + 0.006*\"govt\" + 0.005*\"council\" + 0.005*\"farmer\" + 0.005*\"hick\" + 0.004*\"restrict\" + 0.004*\"plan\"\n",
      "\n",
      "Score: 0.20766673982143402\t \n",
      "Topic: 0.009*\"blaze\" + 0.006*\"plan\" + 0.006*\"firefight\" + 0.006*\"council\" + 0.005*\"govt\" + 0.005*\"polic\" + 0.005*\"crew\" + 0.004*\"call\" + 0.004*\"titl\" + 0.004*\"develop\"\n",
      "\n",
      "Score: 0.020014846697449684\t \n",
      "Topic: 0.008*\"iraq\" + 0.007*\"kill\" + 0.007*\"troop\" + 0.006*\"attack\" + 0.006*\"plan\" + 0.006*\"soldier\" + 0.006*\"govt\" + 0.004*\"polic\" + 0.004*\"test\" + 0.004*\"afghanistan\"\n",
      "\n",
      "Score: 0.02001177705824375\t \n",
      "Topic: 0.005*\"say\" + 0.005*\"govt\" + 0.004*\"fund\" + 0.004*\"council\" + 0.004*\"cancer\" + 0.004*\"open\" + 0.004*\"australia\" + 0.004*\"patient\" + 0.004*\"lead\" + 0.004*\"urg\"\n",
      "\n",
      "Score: 0.020010005682706833\t \n",
      "Topic: 0.007*\"miss\" + 0.006*\"market\" + 0.006*\"search\" + 0.005*\"push\" + 0.005*\"public\" + 0.005*\"rise\" + 0.005*\"record\" + 0.005*\"govt\" + 0.004*\"plan\" + 0.004*\"price\"\n",
      "\n",
      "Score: 0.020009256899356842\t \n",
      "Topic: 0.006*\"polic\" + 0.005*\"assault\" + 0.005*\"govt\" + 0.005*\"station\" + 0.004*\"school\" + 0.004*\"council\" + 0.004*\"plan\" + 0.004*\"nurs\" + 0.004*\"stand\" + 0.004*\"candid\"\n",
      "\n",
      "Score: 0.02000911347568035\t \n",
      "Topic: 0.018*\"crash\" + 0.014*\"polic\" + 0.013*\"kill\" + 0.007*\"investig\" + 0.007*\"charg\" + 0.007*\"court\" + 0.007*\"accid\" + 0.006*\"fatal\" + 0.006*\"bomb\" + 0.006*\"death\"\n",
      "\n",
      "Score: 0.020008733496069908\t \n",
      "Topic: 0.006*\"polic\" + 0.006*\"charg\" + 0.005*\"care\" + 0.005*\"climat\" + 0.005*\"child\" + 0.004*\"seek\" + 0.004*\"violenc\" + 0.004*\"govt\" + 0.004*\"health\" + 0.004*\"bail\"\n",
      "\n",
      "Score: 0.020008621737360954\t \n",
      "Topic: 0.007*\"charg\" + 0.007*\"polic\" + 0.006*\"drive\" + 0.006*\"drink\" + 0.004*\"nuclear\" + 0.004*\"plan\" + 0.004*\"face\" + 0.004*\"govt\" + 0.004*\"kangaroo\" + 0.004*\"right\"\n",
      "\n",
      "Score: 0.02000776119530201\t \n",
      "Topic: 0.006*\"murder\" + 0.006*\"coast\" + 0.006*\"guilti\" + 0.006*\"govt\" + 0.005*\"plead\" + 0.005*\"west\" + 0.004*\"water\" + 0.004*\"urg\" + 0.004*\"plan\" + 0.004*\"charg\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5590149760246277\t Topic: 0.011*\"protest\" + 0.008*\"market\" + 0.008*\"melbourn\" + 0.007*\"prompt\" + 0.007*\"rais\"\n",
      "Score: 0.2809218466281891\t Topic: 0.016*\"health\" + 0.014*\"govt\" + 0.012*\"warn\" + 0.010*\"closer\" + 0.009*\"england\"\n",
      "Score: 0.02001125179231167\t Topic: 0.020*\"plan\" + 0.017*\"water\" + 0.015*\"council\" + 0.012*\"centr\" + 0.009*\"claim\"\n",
      "Score: 0.020009081810712814\t Topic: 0.014*\"hospit\" + 0.014*\"final\" + 0.013*\"test\" + 0.010*\"charg\" + 0.009*\"play\"\n",
      "Score: 0.02000868134200573\t Topic: 0.020*\"council\" + 0.015*\"polic\" + 0.015*\"drug\" + 0.013*\"court\" + 0.012*\"elect\"\n",
      "Score: 0.020007474347949028\t Topic: 0.043*\"polic\" + 0.019*\"kill\" + 0.017*\"investig\" + 0.012*\"attack\" + 0.012*\"probe\"\n",
      "Score: 0.020007459446787834\t Topic: 0.015*\"murder\" + 0.012*\"jail\" + 0.010*\"year\" + 0.010*\"drought\" + 0.009*\"plan\"\n",
      "Score: 0.020007353276014328\t Topic: 0.018*\"miss\" + 0.013*\"search\" + 0.012*\"polic\" + 0.010*\"take\" + 0.008*\"guilti\"\n",
      "Score: 0.020005952566862106\t Topic: 0.023*\"urg\" + 0.017*\"govt\" + 0.013*\"plan\" + 0.011*\"chang\" + 0.010*\"iraq\"\n",
      "Score: 0.02000589855015278\t Topic: 0.021*\"fund\" + 0.019*\"boost\" + 0.019*\"govt\" + 0.011*\"offer\" + 0.011*\"consid\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unseen_document = \"My favorite sports activities are running and swimming.\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
